# -*- coding: utf-8 -*-
"""summary2tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yUe7Eswvgtu36wAq3NIqbnaJUToZONNv
"""


# hyper-params
num_layers = 12
dff = 512
num_heads = 8
EPOCHS = 20
max_len_text = 300
max_len_summ = 25

d_model=256

import pandas as pd
import torch
import random
import numpy as np

train_data=pd.read_csv("/home/mohit/akshay/train.csv",
                 lineterminator='\n')

test_data=pd.read_csv("/home/mohit/akshay/test.csv",
                 lineterminator='\n')



dataset = pd.concat([train_data, test_data], ignore_index=True, sort=False)

dataset=dataset.dropna()

import re
def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, ' ', data)

def preprocess_tokenize(text):
      # for removing punctuation from sentencesc
    text = str(text)
    text = re.sub(r'(\d+)', r'', text)
    text = text.replace('\n', ' ')
    text = text.replace('\r', ' ')
    text = text.replace('\t', ' ')
    text = text.replace('\u200d', '')
    text=re.sub("(__+)", ' ', str(text)).lower()   #remove _ if it occors more than one time consecutively
    text=re.sub("(--+)", ' ', str(text)).lower()   #remove - if it occors more than one time consecutively
    text=re.sub("(~~+)", ' ', str(text)).lower()   #remove ~ if it occors more than one time consecutively
    text=re.sub("(\+\++)", ' ', str(text)).lower()   #remove + if it occors more than one time consecutively
    text=re.sub("(\.\.+)", ' ', str(text)).lower()   #remove . if it occors more than one time consecutively
    text=re.sub(r"[<>()|&©@#ø\[\]\'\",;:?.~*!]", ' ', str(text)).lower() #remove <>()|&©ø"',;?~*!
    text = re.sub(r"[‘’।:]", " ", str(text)) #removing other special characters
    text = re.sub("([a-zA-Z])",' ',str(text)).lower()
    text = re.sub("(\s+)",' ',str(text)).lower()
    text = remove_emojis(text)
    return text

train_data_trg = dataset['summary']
train_data_src = dataset['article']



# train_data_trg = [dataset['summary']]
# train_data_src = [dataset['article']]

tokenized_corpus_trg = [preprocess_tokenize(x) for x in train_data_trg]  #these are headlines
tokenized_corpus_src = [preprocess_tokenize(x) for x in train_data_src]  #these are articles

tokenized_corpus_trg[:2]

dataset['Text_Cleaned'] = tokenized_corpus_src

dataset['Summary_Cleaned'] =  tokenized_corpus_trg

max_text_len = 300
max_summary_len = 25

cleaned_text = np.array(dataset['Text_Cleaned'])
cleaned_summary = np.array(dataset['Summary_Cleaned'])

short_text = []
short_summary = []
for i in range(len(cleaned_text)):
    if(len(cleaned_summary[i].split()) <= max_summary_len
       and len(cleaned_summary[i].split()) > 1
       and len(cleaned_text[i].split()) <= max_text_len):
        short_text.append(cleaned_text[i])
        short_summary.append(cleaned_summary[i])
    elif((len(cleaned_summary[i].split()) > max_summary_len) or (len(cleaned_text[i].split()) > max_text_len)):
        summ = cleaned_summary[i].split()
        txt = cleaned_text[i].split()
        if(len(summ)>max_summary_len):
          short_summary.append(' '.join(summ[:max_summary_len]))
        else:
          short_summary.append(cleaned_summary[i])
        if(len(txt)>max_text_len):
          short_text.append(' '.join(txt[:max_text_len]))
        else:
          short_text.append(cleaned_text[i])

post_pre = pd.DataFrame({'text':short_text,'summary':short_summary})

post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')
post_pre['text'] = post_pre['text'].apply(lambda x: 'sostok ' + x + ' eostok')



cleaned_summary = np.array(post_pre['summary'])
cleaned_text = np.array(post_pre['text'])

single_data=np.concatenate((cleaned_summary, cleaned_text), axis=0)



from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

text_tokenizer = Tokenizer()
text_tokenizer.fit_on_texts(list(single_data))

import random

text_voc=len(text_tokenizer.word_index)+1

text_voc

text__seq    =   text_tokenizer.texts_to_sequences(cleaned_text)
cleaned_text    =   pad_sequences(text__seq,  maxlen=max_text_len, padding='post')


summary__seq    =   text_tokenizer.texts_to_sequences(cleaned_summary)
cleaned_summary    =   pad_sequences(summary__seq,  maxlen=max_summary_len, padding='post')

# cleaned_summary[:2]

"""word2vec"""

# def tokenize_text(text):
#     text=text.strip()
#     sepp=text.split(" ")
#     return sepp

# df_tokenized = post_pre.apply(lambda col: col.apply(tokenize_text), axis=1)

from gensim.models import Word2Vec

embed_model=Word2Vec.load("/media/mohit/custom_word2vec.bin")

# model.wv["संस्कृति"]

# model.wv.most_similar("संस्कृति", topn=3)

# cleaned_text

def create_weight_matrix(model, DICT_SIZE, tokenizer):

#   vector_size = model.wv["नृत्य"].shape[0]
  vector_size=256
  w_matrix = np.zeros((DICT_SIZE, vector_size))

  for word, index in tokenizer.word_index.items():
    if index < DICT_SIZE:
      if word in model.wv.index_to_key:
        w_matrix[index] = model.wv[word]
        # text_words.append(word)
      else:
        w_matrix[index] = np.random.rand(1, vector_size)
        # unk_words.append(word)


  return w_matrix

embed_matrix = create_weight_matrix(embed_model, text_voc, text_tokenizer)

import pandas as pd
import numpy as np
import tensorflow as tf
import keras
import time
import re
import pickle
import matplotlib.pyplot as plt

BUFFER_SIZE = 20000
BATCH_SIZE = 16

# x_train, y_train = cleaned_text[:3, :], cleaned_summary[:3, :]
# x_test, y_test = cleaned_text[3:, :], cleaned_summary[3:, :]

x_train, y_train = cleaned_text, cleaned_summary

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
# test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)

def get_angles(position, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return position * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(
        np.arange(position)[:, np.newaxis],
        np.arange(d_model)[np.newaxis, :],
        d_model
    )

    # apply sin to even indices in the array; 2i
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

    # apply cos to odd indices in the array; 2i+1
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)

def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

def scaled_dot_product_attention(q, k, v, mask):
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    output = tf.matmul(attention_weights, v)
    return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention)

        return output, attention_weights

def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),
        tf.keras.layers.Dense(d_model)
    ])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)


    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)

        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)

        return out3, attn_weights_block1, attn_weights_block2

class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, emb_matrix, max_len, rate=0.1):
        super(Encoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(input_dim = input_vocab_size,
                      output_dim = d_model,
                      input_length = max_len, # max_len of text sequence - 300
                      weights=[emb_matrix],
                      trainable=False) # static weights to be assigned from pretrained embedding
        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)

        return x

class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, emb_matrix, max_len, rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.embedding = tf.keras.layers.Embedding(input_dim = target_vocab_size,
                      output_dim = d_model,
                      input_length = max_len, # max_len of summ sequence - 16
                      weights=[emb_matrix],
                      trainable=False)
        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)

        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}

        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2

        return x, attention_weights

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, text_matrix, max_len_text, summ_matrix, max_len_summ, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, text_matrix, max_len_text, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, summ_matrix, max_len_summ, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
        enc_output = self.encoder(inp, training, enc_padding_mask)

        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)

        final_output = self.final_layer(dec_output)

        return final_output, attention_weights

text_voc



encoder_vocab_size = text_voc
# decoder_vocab_size = summary_voc

d_model = embed_matrix.shape[1]

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        # self.d_model = d_model
        self.d_model = tf.cast(d_model, tf.float32)
        # # self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)

train_loss = tf.keras.metrics.Mean(name='train_loss')

val_loss = tf.keras.metrics.Mean(name='val_loss')

transformer = Transformer(
    num_layers,
    d_model,
    num_heads,
    dff,
    encoder_vocab_size,
    encoder_vocab_size,
    pe_input = encoder_vocab_size,
    pe_target = encoder_vocab_size,
    text_matrix = embed_matrix,
    max_len_text = max_len_text,
    summ_matrix = embed_matrix,
    max_len_summ = max_len_summ
)

def create_masks(inp, tar):
    enc_padding_mask = create_padding_mask(inp)
    dec_padding_mask = create_padding_mask(inp)

    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp, tar_inp,
            True,
            enc_padding_mask,
            combined_mask,
            dec_padding_mask
        )
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    train_loss(loss)

@tf.function
def val_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(
            inp, tar_inp,
            False,
            enc_padding_mask,
            combined_mask,
            dec_padding_mask
        )
        loss = loss_function(tar_real, predictions)
    val_loss(loss)



train_loss_values = []
test_loss_values = []
for epoch in range(EPOCHS):
    start = time.time()

    train_loss.reset_states()
    val_loss.reset_states()
    for (batch, (inp, tar)) in enumerate(dataset):
        train_step(inp, tar)

        if batch % 500 == 0:
            print ('Epoch {} ========> Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result().numpy()))


    train_loss_values.append(train_loss.result().numpy())

    #test set - forward pass, loss fn (no backward pass)
    # for (batch, (inp, tar)) in enumerate(test_dataset):
    #     val_step(inp, tar)

    test_loss_values.append(val_loss.result().numpy())

    print ('Epoch {}/{} ========> Training Loss {:.4f} Test/Validation Loss {:.4f}'.format(epoch + 1, EPOCHS, train_loss.result().numpy(), val_loss.result().numpy()))

    print ('Time taken for epoch {}: {:.2f} secs\n'.format(epoch+1, time.time() - start))

# no_of_epochs = [i for i in range(EPOCHS)]
# fig , ax = plt.subplots(1,1)
# fig.set_size_inches(10,6)
# ax.plot(no_of_epochs, test_loss_values, label = 'Testing Loss')
# ax.plot(no_of_epochs, train_loss_values, label = 'Training Loss')
# ax.set_title('Categorical Crossentropy Loss vs No. of Epochs')
# ax.legend()
# ax.set_xlabel("No. of Epochs")
# plt.show()

max_text_len = 300
max_summary_len = 25

def predict(input_document):
    # #clean
    input_document = preprocess_tokenize(input_document)
    input_document = "sostok "+input_document+" eostok"
    #tokenize
    input_document = text_tokenizer.texts_to_sequences([input_document])
    #padding
    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=max_text_len, padding='post', truncating='post')

    encoder_input = tf.expand_dims(input_document[0], 0)

    decoder_input = [text_tokenizer.word_index["sostok"]]
    output = tf.expand_dims(decoder_input, 0)

    for i in range(max_summary_len):
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)

        predictions, attention_weights = transformer(
            encoder_input,
            output,
            False,
            enc_padding_mask,
            combined_mask,
            dec_padding_mask,
        )

        predictions = predictions[: ,-1:, :]
        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

        if predicted_id == text_tokenizer.word_index["eostok"]:
            return tf.squeeze(output, axis=0), attention_weights

        output = tf.concat([output, predicted_id], axis=-1)

    return tf.squeeze(output, axis=0), attention_weights

text_tokenizer.word_index["sostok"]

encoder_input = tf.expand_dims(x_train[1], 0)

decoder_input = [text_tokenizer.word_index["sostok"]]
output = tf.expand_dims(decoder_input, 0)
for i in range(max_summary_len):
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)

    predictions, attention_weights = transformer(
        encoder_input,
        output,
        False,
        enc_padding_mask,
        combined_mask,
        dec_padding_mask,
    )

    predictions = predictions[: ,-1:, :]
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    output = tf.concat([output, predicted_id], axis=-1)
    summarized=tf.squeeze(output, axis=0)
    summarized = np.expand_dims(summarized[1:], 0)

    text_tokenizer.sequences_to_texts(summarized)[0]

def summarize(input_document):
    # not considering attention weights for now, can be used to plot attention heatmaps in the future
    summarized = predict(input_document=input_document)[0].numpy()
    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token
    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document

print(summarize("पूर्व प्रधानमंत्री और कांग्रेस के दिग्गज नेता राजीव गांधी की आज 75वीं जयंती है. इस मौके पर कांग्रेस अध्यक्ष राहुल गांधी, यूपीए चेयरपर्सन सोनिया गांधी, प्रियंका गांधी, पूर्व प्रधानमंत्री मनमोहन सिंह समेत कई बड़े कांग्रेस नेताओं ने उन्हें श्रद्धांजलि अर्पित की. प्रधानमंत्री नरेंद्र मोदी ने भी ट्वीट कर उन्हें श्रद्धांजलि दी. बता दें कि राजीव गांधी का जन्म आज ही के दिन 1944 में हुआ था. 21 मई, 1991 को उनकी हत्या कर दी गई थी. राजीव गांधी 1984 से 1989 तक प्रधानमंत्री रहे. तमिलनाडु के श्रीपेरंबुदुर में 21 मई, 1991 को आम चुनाव के प्रचार के दौरान एलटीटीई के एक आत्मघाती हमलावर ने राजीव गांधी की हत्या कर दी थी. 1984 में इंदिरा गांधी की हत्या के बाद वह भारी बहुमत के साथ प्रधानमंत्री बने. राजीव गांधी की राजनीति में कोई रूचि नहीं थी और वो एक एयरलाइन पायलट की नौकरी करते थे और उसी में खुश थे. लेकिन आपातकाल के उपरान्त जब इंदिरा गांधी को सत्ता छोड़नी पड़ी थी. वहीं साल 1980 में छोटे भाई संजय गांधी की हवाई जहाज दुर्घटना में मृत्यु हो जाने के बाद माता इंदिरा का सहयोग देने के लिए उन्होंने राजनीति में प्रवेश कर लिया."))

print(summarize('सूत्रों के मुताबिक मंगलवार को दिनहाटा इलाके में पार्टी कार्यक्रम से घर जाते समय केंद्रीय गृह राज्य मंत्री निशिथ प्रमाणिक के सुरक्षा गार्डों के साथ तृणमूल कार्यकर्ताओं की झड़प हो गई। स्थिति को नियंत्रित करने की कोशिश में दिनहाटा उपमंडल पुलिस अधिकारी धीमान मित्रा और कई अन्य घायल हो गए। पश्चिम बंगाल के कूचबिहार जिले के दिनहाटा में मंगलवार को तृणमूल कांग्रेस और बीजेपी समर्थकों के बीच झड़प से इलाके में तनाव फैल गया। उत्तर बंगाल विकास मंत्री उदयन गुहा और केंद्रीय गृह राज्य मंत्री निशिथ प्रमाणिक ने एक-दूसरे के समर्थकों पर हमले का आरोप लगाया है। स्थानीय सूत्रों के अनुसार, राजनीतिक संघर्ष में दोनों पक्षों के कई लोग घायल हुए हैं। साथ ही दिनहाटा के एसडीपीओ को भी चोट आई है। हालांकि बताया जा रहा है कि दोनों ही मंत्री सुरक्षित हैं।'))

